{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import random\n",
    "import pdb\n",
    "\n",
    "f = h5py.File('data/Full_NF.h5', 'r')\n",
    "dset = f['nf_data']\n",
    "dset = dset[:]\n",
    "print(\"Data loaded from file\")\n",
    "\n",
    "dset_indicies = set(range(len(dset)))\n",
    "train_num = int(.9 * len(dset))\n",
    "print(\"Training Samples: \", train_num, \"| Validation Samples: \", len(dset) - train_num)\n",
    "\n",
    "train_indicies = random.sample(dset_indicies, train_num)\n",
    "validation_indicies = dset_indicies.difference(train_indicies)\n",
    "training_data = [dset[i] for i in train_indicies]\n",
    "validation_data = [dset[i] for i in validation_indicies]\n",
    "print(\"Training and Validation datasets separated\")\n",
    "\n",
    "del dset_indicies\n",
    "del train_indicies\n",
    "del validation_indicies\n",
    "del dset\n",
    "        \n",
    "def nf_label_transformation_function(label):\n",
    "    \n",
    "    one_hot = np.zeros(5)\n",
    "    one_hot[label - 1] = 1\n",
    "    return one_hot\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "import keras\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from generator_helper_functions import input_layer, naive_input_size , Generate_Naive_Feedforward_Contig, Generate_Naive_Feedforward\n",
    "\n",
    "#opt = keras.optimizers.rmsprop()\n",
    "opt = keras.optimizers.SGD()\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(128, activation='relu', input_shape=(naive_input_size,)))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(5, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "             optimizer=opt,\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "print(\"Model Compiled\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen = Generate_Naive_Feedforward(training_data, \n",
    "                                batch_size, \n",
    "                                (naive_input_size,), \n",
    "                                (5,), #label shape\n",
    "                                input_layer, \n",
    "                                lambda x: nf_label_transformation_function(x[2]),\n",
    "                                normalized = True\n",
    "                                )\n",
    "\n",
    "validation_gen = Generate_Naive_Feedforward(validation_data, \n",
    "                                batch_size, \n",
    "                                (naive_input_size,), \n",
    "                                (5,), #label shape\n",
    "                                input_layer, \n",
    "                                lambda x: nf_label_transformation_function(x[2]),\n",
    "                                normalized = True\n",
    "                                )\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='models/naive_feedforward_SGD_128_128v1_normalized.hdf5', verbose=1, save_best_only=True)\n",
    "model.fit_generator(train_gen,\n",
    "                    epochs=20,\n",
    "                    steps_per_epoch=1000,\n",
    "                    validation_data=validation_gen,\n",
    "                    validation_steps=100,\n",
    "                    callbacks=[checkpointer]\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import random\n",
    "import pdb\n",
    "\n",
    "f = h5py.File('data/Full_NF.h5', 'r')\n",
    "dset = f['nf_data']\n",
    "dset = dset[:]\n",
    "print(\"Data loaded from file\")\n",
    "\n",
    "dset_indicies = set(range(len(dset)))\n",
    "train_num = int(.9 * len(dset))\n",
    "print(\"Training Samples: \", train_num, \"| Validation Samples: \", len(dset) - train_num)\n",
    "\n",
    "train_indicies = random.sample(dset_indicies, train_num)\n",
    "validation_indicies = dset_indicies.difference(train_indicies)\n",
    "training_data = [dset[i] for i in train_indicies]\n",
    "validation_data = [dset[i] for i in validation_indicies]\n",
    "print(\"Training and Validation datasets separated\")\n",
    "\n",
    "del dset_indicies\n",
    "del train_indicies\n",
    "del validation_indicies\n",
    "del dset\n",
    "\n",
    "def Generate_Naive_Feedforward(dataset, batch_size, feature_shape, feature_transformation_function, label_transformation_function):\n",
    "    \n",
    "    features = np.zeros((batch_size, *feature_shape))\n",
    "    labels = np.zeros((batch_size, 5))\n",
    "    index_set = set(range(len(dataset)))\n",
    "    \n",
    "    while True:\n",
    "            \n",
    "        batch_indicies = []    \n",
    "        if (len(index_set) < batch_size):\n",
    "            index_set = set(range(len(dataset)))\n",
    "            batch_indicies = random.sample(index_set, batch_size)\n",
    "        else:\n",
    "            batch_indicies = random.sample(index_set, batch_size)\n",
    "            index_set = index_set.difference(batch_indicies)\n",
    "        \n",
    "        for i, batch_index in enumerate(batch_indicies):\n",
    "            features[i] = feature_transformation_function(dataset[batch_index])\n",
    "            labels[i] = label_transformation_function(dataset[batch_index])\n",
    "            \n",
    "        yield (features, labels)\n",
    "        \n",
    "def nf_label_transformation_function(label):\n",
    "    \n",
    "    one_hot = np.zeros(5)\n",
    "    one_hot[label - 1] = 1\n",
    "    return one_hot\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "import keras\n",
    "from keras.models import load_model\n",
    "from generator_helper_functions import input_layer, naive_input_size\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "model = load_model('models/naive_feedforward_SGD_128_128v1.hdf5')\n",
    "\n",
    "print(\"Loaded\")\n",
    "\n",
    "validation_gen = Generate_Naive_Feedforward(validation_data, \n",
    "                                batch_size, \n",
    "                                (naive_input_size,), \n",
    "                                input_layer, \n",
    "                                lambda x: nf_label_transformation_function(x[2]))\n",
    "\n",
    "model.evaluate_generator(validation_gen,\n",
    "                    steps=1000,\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying different neurons per hidden layer and regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import random\n",
    "import pdb\n",
    "\n",
    "f = h5py.File('data/Full_NF.h5', 'r')\n",
    "dset = f['nf_data']\n",
    "dset = dset[:]\n",
    "print(\"Data loaded from file\")\n",
    "\n",
    "dset_indicies = set(range(len(dset)))\n",
    "train_num = int(.9 * len(dset))\n",
    "print(\"Training Samples: \", train_num, \"| Validation Samples: \", len(dset) - train_num)\n",
    "\n",
    "train_indicies = random.sample(dset_indicies, train_num)\n",
    "validation_indicies = dset_indicies.difference(train_indicies)\n",
    "training_data = [dset[i] for i in train_indicies]\n",
    "validation_data = [dset[i] for i in validation_indicies]\n",
    "print(\"Training and Validation datasets separated\")\n",
    "\n",
    "del dset_indicies\n",
    "del train_indicies\n",
    "del validation_indicies\n",
    "del dset\n",
    "\n",
    "def Generate_Naive_Feedforward(dataset, batch_size, feature_shape, feature_transformation_function, label_transformation_function, normalized = False):\n",
    "    \n",
    "    features = np.zeros((batch_size, *feature_shape))\n",
    "    labels = np.zeros((batch_size,))\n",
    "    index_set = set(range(len(dataset)))\n",
    "    \n",
    "    while True:\n",
    "            \n",
    "        batch_indicies = []    \n",
    "        if (len(index_set) < batch_size):\n",
    "            index_set = set(range(len(dataset)))\n",
    "            batch_indicies = random.sample(index_set, batch_size)\n",
    "        else:\n",
    "            batch_indicies = random.sample(index_set, batch_size)\n",
    "            index_set = index_set.difference(batch_indicies)\n",
    "        \n",
    "        for i, batch_index in enumerate(batch_indicies):\n",
    "            features[i] = feature_transformation_function(dataset[batch_index], normalized)\n",
    "            labels[i] = label_transformation_function(dataset[batch_index])\n",
    "            \n",
    "        yield (features, labels)\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense, Lambda\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from generator_helper_functions import input_layer, naive_input_size\n",
    "\n",
    "#opt = keras.optimizers.rmsprop()\n",
    "opt = keras.optimizers.SGD()\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "def round(inp):\n",
    "    return tf.round(inp * 4) / 4\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(90, activation='relu', input_shape=(naive_input_size,)))\n",
    "model.add(Dense(90, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.add(Lambda(round))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "             optimizer=opt,\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "print(\"Model Compiled\")\n",
    "\n",
    "train_gen = Generate_Naive_Feedforward(training_data, \n",
    "                                batch_size, \n",
    "                                (naive_input_size,), \n",
    "                                input_layer, \n",
    "                                lambda x: (x[2] - 1)/4,\n",
    "                                normalized=True)\n",
    "\n",
    "validation_gen = Generate_Naive_Feedforward(validation_data, \n",
    "                                batch_size, \n",
    "                                (naive_input_size,), \n",
    "                                input_layer, \n",
    "                                lambda x: (x[2] - 1 )/4,\n",
    "                                normalized=True)\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='models/naive_feedforward_regression_90_90_normalized.hdf5', verbose=1, save_best_only=True)\n",
    "model.fit_generator(train_gen,\n",
    "                    epochs=1,\n",
    "                    steps_per_epoch=1000,\n",
    "                    validation_data=validation_gen,\n",
    "                    validation_steps=100,\n",
    "                    callbacks=[checkpointer]\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(input_layer(training_data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer(training_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying different neurons per hidden layer and poisson regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('../data/validation_data.pickle', 'rb') as handle:\n",
    "    validation_data = pickle.load(handle)\n",
    "\n",
    "with open('../data/training_data.pickle', 'rb') as handle:\n",
    "    training_data = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len (validation_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/steich/NF_Prize_Data/nf_prize_env/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and Validation datasets separated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Compiled\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 2030s 2s/step - loss: 0.0750 - acc: 0.2238 - val_loss: 0.0632 - val_acc: 0.1695\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.06321, saving model to ../models/naive_feedforward_regression_poisson_Native_64_932.hdf5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f8ec32cda58>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import random\n",
    "import pdb\n",
    "import pickle\n",
    "\n",
    "# f = h5py.File('../data/Full_NF.h5', 'r')\n",
    "# dset = f['nf_data']\n",
    "# dset = dset[:]\n",
    "# print(\"Data loaded from file\")\n",
    "\n",
    "# dset_indicies = set(range(len(dset)))\n",
    "# train_num = int(.9 * len(dset))\n",
    "# print(\"Training Samples: \", train_num, \"| Validation Samples: \", len(dset) - train_num)\n",
    "\n",
    "# train_indicies = random.sample(dset_indicies, train_num)\n",
    "# validation_indicies = dset_indicies.difference(train_indicies)\n",
    "# training_data = [dset[i] for i in train_indicies]\n",
    "# validation_data = [dset[i] for i in validation_indicies]\n",
    "# print(\"Training and Validation datasets separated\")\n",
    "\n",
    "with open('../data/validation_data.pickle', 'rb') as handle:\n",
    "    validation_data = pickle.load(handle)\n",
    "\n",
    "with open('../data/training_data.pickle', 'rb') as handle:\n",
    "    training_data = pickle.load(handle)\n",
    "\n",
    "print(\"Training and Validation datasets separated\")\n",
    "\n",
    "\n",
    "# del dset_indicies\n",
    "# del train_indicies\n",
    "# del validation_indicies\n",
    "# del dset\n",
    "\n",
    "# def Generate_Naive_Feedforward(dataset, batch_size, feature_shape, feature_transformation_function, label_transformation_function):\n",
    "    \n",
    "#     features = np.zeros((batch_size, *feature_shape))\n",
    "#     labels = np.zeros((batch_size,))\n",
    "#     index_set = set(range(len(dataset)))\n",
    "    \n",
    "#     while True:\n",
    "            \n",
    "#         batch_indicies = []    \n",
    "#         if (len(index_set) < batch_size):\n",
    "#             index_set = set(range(len(dataset)))\n",
    "#             batch_indicies = random.sample(index_set, batch_size)\n",
    "#         else:\n",
    "#             batch_indicies = random.sample(index_set, batch_size)\n",
    "#             index_set = index_set.difference(batch_indicies)\n",
    "        \n",
    "#         for i, batch_index in enumerate(batch_indicies):\n",
    "#             features[i] = feature_transformation_function(dataset[batch_index])\n",
    "#             labels[i] = label_transformation_function(dataset[batch_index])\n",
    "            \n",
    "#         yield (features, labels)\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "import keras\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from generator_helper_functions import input_layer, naive_input_size, Generate_Naive_Feedforward\n",
    "\n",
    "#opt = keras.optimizers.rmsprop()\n",
    "opt = keras.optimizers.SGD()\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(512, activation='relu', input_shape=(int(naive_input_size),)))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(1, activation='relu'))\n",
    "\n",
    "model.compile(loss='mean_squared_error',\n",
    "             optimizer=opt,\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "print(\"Model Compiled\")\n",
    "\n",
    "train_gen = Generate_Naive_Feedforward(training_data, \n",
    "                                batch_size, \n",
    "                                (naive_input_size,), \n",
    "                                (1,),\n",
    "                                input_layer, \n",
    "                                lambda x: (x[2] - 1)/4,\n",
    "                                normalized=True)\n",
    "\n",
    "validation_gen = Generate_Naive_Feedforward(validation_data, \n",
    "                                batch_size, \n",
    "                                (naive_input_size,),\n",
    "                                (1,),\n",
    "                                input_layer, \n",
    "                                lambda x: (x[2] - 1 )/4,\n",
    "                                normalized=True)\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='../models/naive_feedforward_regression_poisson_Native_64_932.hdf5', verbose=1, save_best_only=True)\n",
    "model.fit_generator(train_gen,\n",
    "                    epochs=1,\n",
    "                    steps_per_epoch=1000,\n",
    "                    validation_data=validation_gen,\n",
    "                    validation_steps=10,\n",
    "                    callbacks=[checkpointer]\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.33366666666666667"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_score(\n",
    "    list(map(lambda x: round(float(model.predict(np.expand_dims(input_layer(x,   True), axis=0)) * 4 + 1)), \n",
    "             validation_data[0:3000])), \n",
    "    validation_data[0:3000,2], average='micro') \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import random\n",
    "import pdb\n",
    "\n",
    "f = h5py.File('data/Full_NF.h5', 'r')\n",
    "dset = f['nf_data']\n",
    "dset = dset[:]\n",
    "print(\"Data loaded from file\")\n",
    "\n",
    "dset_indicies = set(range(len(dset)))\n",
    "train_num = int(.9 * len(dset))\n",
    "print(\"Training Samples: \", train_num, \"| Validation Samples: \", len(dset) - train_num)\n",
    "\n",
    "train_indicies = random.sample(dset_indicies, train_num)\n",
    "validation_indicies = dset_indicies.difference(train_indicies)\n",
    "training_data = [dset[i] for i in train_indicies]\n",
    "validation_data = [dset[i] for i in validation_indicies]\n",
    "print(\"Training and Validation datasets separated\")\n",
    "\n",
    "del dset_indicies\n",
    "del train_indicies\n",
    "del validation_indicies\n",
    "del dset\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "import keras\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from generator_helper_functions import input_layer, naive_input_size, Generate_Naive_Feedforward_Contig\n",
    "\n",
    "#opt = keras.optimizers.rmsprop()\n",
    "opt = keras.optimizers.SGD()\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(90, activation='relu', input_shape=(naive_input_size,)))\n",
    "model.add(Dense(90, activation='relu'))\n",
    "model.add(Dense(1, activation='relu'))\n",
    "\n",
    "model.compile(loss='poisson',\n",
    "             optimizer=opt,\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "print(\"Model Compiled\")\n",
    "\n",
    "train_gen = Generate_Naive_Feedforward_Contig(training_data, \n",
    "                                batch_size, \n",
    "                                (naive_input_size,), \n",
    "                                input_layer, \n",
    "                                lambda x: (x[2] - 1)/4)\n",
    "\n",
    "validation_gen = Generate_Naive_Feedforward_Contig(validation_data, \n",
    "                                batch_size, \n",
    "                                (naive_input_size,), \n",
    "                                input_layer, \n",
    "                                lambda x: (x[2] - 1 )/4)\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='models/naive_feedforward_regression_poisson_90_90.hdf5', verbose=1, save_best_only=True)\n",
    "model.fit_generator(train_gen,\n",
    "                    epochs=1,\n",
    "                    steps_per_epoch=500,\n",
    "                    validation_data=validation_gen,\n",
    "                    validation_steps=10,\n",
    "                    callbacks=[checkpointer]\n",
    "                    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nf_prize_data",
   "language": "python",
   "name": "nf_prize_data"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
