{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cody/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import pickle\n",
    "import random\n",
    "import pdb\n",
    "from generator_helper_functions import create_user_maxtrix, create_user_ids, latent_user_input_layer\n",
    "\n",
    "# f = h5py.File('../data/Full_NF.h5', 'r')\n",
    "# dset = f['dataset_1']\n",
    "# dset = dset[:50000000]\n",
    "# print(\"Data loaded from file\")\n",
    "\n",
    "# dset_indicies = set(range(len(dset)))\n",
    "# train_num = int(.9 * len(dset))\n",
    "# print(\"Training Samples: \", train_num, \"| Validation Samples: \", len(dset) - train_num)\n",
    "\n",
    "# user_ids = create_user_ids(dset)\n",
    "# train_indicies = random.sample(dset_indicies, train_num)\n",
    "# validation_indicies = dset_indicies.difference(train_indicies)\n",
    "# training_data = np.array([dset[i] for i in train_indicies])\n",
    "# del train_indicies\n",
    "# validation_data = np.array([dset[i] for i in validation_indicies])\n",
    "# del validation_indicies\n",
    "# print(\"Training and Validation datasets separated\")\n",
    "# #del dset\n",
    "\n",
    "with open('../data/training_data.pickle', 'rb') as handle:\n",
    "    training_data = pickle.load(handle)\n",
    "    \n",
    "hf = h5py.File('../data/training_data.h5', 'w')\n",
    "hf.create_dataset('training_data', data=training_data)\n",
    "del training_data\n",
    "training_data = hf[\"training_data\"]\n",
    "del hf\n",
    "\n",
    "    \n",
    "with open('../data/validation_data.pickle', 'rb') as handle:\n",
    "    validation_data = pickle.load(handle)\n",
    "    \n",
    "hf = h5py.File('../data/validation_data.h5', 'w')\n",
    "hf.create_dataset('validation_data', data=validation_data)\n",
    "del validation_data\n",
    "validation_data = hf[\"validation_data\"]\n",
    "del hf\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = h5py.File('../data/Full_NF.h5', 'r')\n",
    "dset = f['dataset_1']\n",
    "dset = dset[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset_mean = np.mean(dset[:,2])\n",
    "del dset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "f = h5py.File('../data/Full_NF.h5', 'r')\n",
    "dset = f['dataset_1']\n",
    "dset = dset[:100]\n",
    "user_ids = create_user_ids(dset)\n",
    "k = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_indices[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#del dset_indicies\n",
    "#del train_indicies\n",
    "#del validation_indicies\n",
    "del dset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-57279f1b780f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0muser_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_user_maxtrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muser_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk_n\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/NF_Prize_Data_Project/notebooks/generator_helper_functions.py\u001b[0m in \u001b[0;36mcreate_user_maxtrix\u001b[0;34m(training_data, user_ids, k_n)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;31m# return reduced dim matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mU\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "user_matrix = create_user_maxtrix(training_data[:], user_ids, k_n=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_matrix[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('../data/user_matrix.pickle', 'wb') as handle:\n",
    "#     pickle.dump(user_matrix, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "# with open('../data/training_data.pickle', 'wb') as handle:\n",
    "#     pickle.dump(training_data, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "# with open('../data/validation_data.pickle', 'wb') as handle:\n",
    "#     pickle.dump(validation_data, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open('../data/user_ids.pickle', 'wb') as handle:\n",
    "    pickle.dump(user_ids, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del user_matrix\n",
    "del training_data\n",
    "del validation_data\n",
    "del user_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf = h5py.File('../data/training_data.h5', 'r')\n",
    "del training_data\n",
    "training_data = hf[\"training_data\"]\n",
    "del hf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import h5py\n",
    "with open('../data/user_matrix.pickle', 'rb') as handle:\n",
    "    user_matrix = pickle.load(handle)\n",
    "# with open('../data/training_data.pickle', 'rb') as handle:\n",
    "#     training_data = pickle.load(handle)\n",
    "# with open('../data/validation_data.pickle', 'rb') as handle:\n",
    "#     validation_data = pickle.load(handle)\n",
    "with open('../data/user_ids.pickle', 'rb') as handle:\n",
    "    user_ids = pickle.load(handle)\n",
    "# hf = h5py.File('../data/training_data.h5', 'r')\n",
    "# training_data = hf[\"training_data\"]\n",
    "# del hf\n",
    "# hf = h5py.File('../data/validation_data.h5', 'r')\n",
    "# validation_data = hf[\"validation_data\"]\n",
    "# del hf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from generator_helper_functions import create_user_maxtrix\n",
    "user_matrix = create_user_maxtrix(training_data, user_ids, k_n=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = MinMaxScaler(copy=False)\n",
    "transformer.fit_transform(user_matrix)\n",
    "user_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 50      \n",
    "def nf_label_transformation_function(label):\n",
    "    \n",
    "    one_hot = np.zeros(5)\n",
    "    one_hot[label - 1] = 1\n",
    "    return one_hot\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "import keras\n",
    "#import random\n",
    "import numpy as np\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from generator_helper_functions import  latent_user_input_layer, Generate_User_Latent_Feedforward\n",
    "\n",
    "#opt = keras.optimizers.rmsprop()\n",
    "opt = keras.optimizers.SGD()\n",
    "\n",
    "\n",
    "# k dim user vector + number of movies + statistics + month, day, year\n",
    "input_size = k + 17770\n",
    "\n",
    "batch_size = 2\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(input_size, activation='relu', input_shape=(input_size,)))\n",
    "model.add(Dense(1000, activation='relu'))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(10, activation='relu'))\n",
    "model.add(Dense(1, activation='linear'))\n",
    "\n",
    "model.compile(loss='mse',\n",
    "             optimizer='sgd',\n",
    "             metrics=['mae', 'acc'])\n",
    "\n",
    "print(\"Model Compiled\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "k = 50        \n",
    "def nf_label_transformation_function(label):\n",
    "    \n",
    "    one_hot = np.zeros(5)\n",
    "    one_hot[label - 1] = 1\n",
    "    return one_hot\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "import keras\n",
    "#import random\n",
    "import numpy as np\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from generator_helper_functions import  latent_user_input_layer, Generate_User_Latent_Feedforward\n",
    "\n",
    "#opt = keras.optimizers.rmsprop()\n",
    "opt = keras.optimizers.SGD()\n",
    "\n",
    "\n",
    "# k dim user vector + number of movies + statistics + month, day, year\n",
    "input_size = k + 17770\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(input_size//2, activation='relu', input_shape=(input_size,)))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(1, activation='linear'))\n",
    "\n",
    "model.compile(loss='mae',\n",
    "             optimizer='sgd',\n",
    "             metrics=['mse', 'acc'])\n",
    "\n",
    "print(\"Model Compiled\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "input_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_gen = Generate_User_Latent_Feedforward(training_data, \n",
    "                                batch_size, \n",
    "                                (input_size,), \n",
    "                                (1,), #label shape\n",
    "                                latent_user_input_layer, \n",
    "                                lambda x: x[2],\n",
    "                                user_matrix,\n",
    "                                user_ids\n",
    "                                )\n",
    "\n",
    "validation_gen = Generate_User_Latent_Feedforward(validation_data, \n",
    "                                batch_size, \n",
    "                                (input_size,), \n",
    "                                (1,), #label shape\n",
    "                                latent_user_input_layer, \n",
    "                                lambda x: x[2],\n",
    "                                user_matrix,\n",
    "                                user_ids\n",
    "                                )\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='../models/latent_user_mse_K50_regression_v1.hdf5', verbose=1, save_best_only=True)\n",
    "model.fit_generator(train_gen,\n",
    "                    epochs=2,\n",
    "                    steps_per_epoch=1000,\n",
    "                    validation_data=validation_gen,\n",
    "                    validation_steps=100,\n",
    "                    callbacks=[checkpointer]\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('../models/latent_user_SGD_MSE_K50_cat_v2_normalized.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "list(map(lambda x: float(model.predict(np.expand_dims(latent_user_input_layer(x, user_matrix, user_ids, True), axis=0)) * 4 + 1), \n",
    "             validation_data[0:30]))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_data[0:30, 2]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "dset = f['nf_data']\n",
    "dset = dset[:]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "dset[:,1][0:3]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "user_ids = list(set(dset[:,1]))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "del dset"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "movie_ids = list(range(0, 17771))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "training_data[training_data[:,1].argsort()]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "movie_ids[-1]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from scipy.sparse import *\n",
    "from scipy import *\n",
    "user_rating_matrix = lil_matrix( (len(user_ids), 17771), dtype=int8 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_ids = {user:i for i, user in enumerate(user_ids)}"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for i in training_data:\n",
    "    user_rating_matrix[user_ids[i[1]], i[0]] = i[2]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def add_user_rating(i):\n",
    "    user_rating_matrix[user_ids[i[1]], i[0]] = i[2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_rating_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_rating_matrix"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%%capture\n",
    "list(map(add_user_rating, training_data[:]))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "user_rating_matrix"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "sort_test = training_data[0:10]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from scipy.sparse.linalg import svds"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "user_rating_matrix = user_rating_matrix.tocsr()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "U, sigma, Vt = svds(user_rating_matrix.asfptype(), k = 50)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "user_group_vectors = U * sigma"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "user_group_vectors[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
