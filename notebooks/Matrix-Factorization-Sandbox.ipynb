{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import pickle\n",
    "import random\n",
    "import pdb\n",
    "from generator_helper_functions import create_user_maxtrix, create_user_ids, create_movie_matrix, latent_user_input_layer\n",
    "\n",
    "\n",
    "with open('../data/training_data.pickle', 'rb') as handle:\n",
    "    training_data = pickle.load(handle)\n",
    "    \n",
    "hf = h5py.File('../data/training_data.h5', 'w')\n",
    "hf.create_dataset('training_data', data=training_data)\n",
    "del training_data\n",
    "training_data = hf[\"training_data\"]\n",
    "del hf\n",
    "\n",
    "    \n",
    "with open('../data/validation_data.pickle', 'rb') as handle:\n",
    "    validation_data = pickle.load(handle)\n",
    "    \n",
    "hf = h5py.File('../data/validation_data.h5', 'w')\n",
    "hf.create_dataset('validation_data', data=validation_data)\n",
    "del validation_data\n",
    "validation_data = hf[\"validation_data\"]\n",
    "del hf\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "f = h5py.File('../data/Full_NF.h5', 'r')\n",
    "dset = f['dataset_1']\n",
    "dset = dset[:100]\n",
    "user_ids = create_user_ids(dset)\n",
    "k = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_indices[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#del dset_indicies\n",
    "#del train_indicies\n",
    "#del validation_indicies\n",
    "del dset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_matrix = create_user_maxtrix(training_data[:], user_ids, k_n=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_users = len(user_ids)\n",
    "print(num_users)\n",
    "movie_matrix = create_movie_matrix(training_data[:], user_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_matrix[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del validation_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('../data/user_matrix.pickle', 'wb') as handle:\n",
    "#     pickle.dump(user_matrix, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "# with open('../data/training_data.pickle', 'wb') as handle:\n",
    "#     pickle.dump(training_data, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "# with open('../data/validation_data.pickle', 'wb') as handle:\n",
    "#     pickle.dump(validation_data, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "# with open('../data/user_ids.pickle', 'wb') as handle:\n",
    "#     pickle.dump(user_ids, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open('../data/movie_matrix.pickle', 'wb') as handle:\n",
    "    pickle.dump(movie_matrix, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del user_matrix\n",
    "del training_data\n",
    "del validation_data\n",
    "del user_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf = h5py.File('../data/training_data.h5', 'r')\n",
    "del training_data\n",
    "training_data = hf[\"training_data\"]\n",
    "del hf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import h5py\n",
    "with open('../data/user_matrix.pickle', 'rb') as handle:\n",
    "    user_matrix = pickle.load(handle)\n",
    "# with open('../data/training_data.pickle', 'rb') as handle:\n",
    "#     training_data = pickle.load(handle)\n",
    "# with open('../data/validation_data.pickle', 'rb') as handle:\n",
    "#     validation_data = pickle.load(handle)\n",
    "with open('../data/user_ids.pickle', 'rb') as handle:\n",
    "    user_ids = pickle.load(handle)\n",
    "# hf = h5py.File('../data/training_data.h5', 'r')\n",
    "# training_data = hf[\"training_data\"]\n",
    "# del hf\n",
    "# hf = h5py.File('../data/validation_data.h5', 'r')\n",
    "# validation_data = hf[\"validation_data\"]\n",
    "# del hf\n",
    "with open('../data/movie_matrix.pickle', 'rb') as handle:\n",
    "    movie_matrix = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from generator_helper_functions import create_user_maxtrix\n",
    "user_matrix = create_user_maxtrix(training_data, user_ids, k_n=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = MinMaxScaler(copy=False)\n",
    "transformer.fit_transform(user_matrix)\n",
    "user_matrix\n",
    "transformer.fit_transform(movie_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MF model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 50      \n",
    "def nf_label_transformation_function(label):\n",
    "    \n",
    "    one_hot = np.zeros(5)\n",
    "    one_hot[label - 1] = 1\n",
    "    return one_hot\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "import keras\n",
    "#import random\n",
    "import numpy as np\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from generator_helper_functions import  latent_user_input_layer, Generate_MF_Feedforward, MF_Layer\n",
    "\n",
    "#opt = keras.optimizers.rmsprop()\n",
    "opt = keras.optimizers.SGD()\n",
    "\n",
    "\n",
    "# k dim user vector + number of movies + statistics + month, day, year\n",
    "input_size = k + 75\n",
    "\n",
    "batch_size = 3\n",
    "\n",
    "# model = Sequential()\n",
    "# model.add(Dense(input_size, activation='relu', input_shape=(input_size,)))\n",
    "# model.add(Dense(input_size, activation='relu'))\n",
    "# model.add(Dense(input_size // 2, activation='relu'))\n",
    "# model.add(Dense(input_size // 4, activation='relu'))\n",
    "# model.add(Dense(10, activation='relu'))\n",
    "# model.add(Dense(5, activation='softmax'))\n",
    "\n",
    "# model.compile(loss='categorical_crossentropy',\n",
    "#              optimizer='sgd',\n",
    "#              metrics=['mae', 'acc'])\n",
    "\n",
    "# print(\"Model Compiled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"length of data is \", len(training_data)//8)\n",
    "print(\"iterations is \", (len(training_data)//8) * 2 - (len(training_data)//8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MF_training_data = np.zeros((len(training_data)//8,125), dtype=float)\n",
    "MF_training_data_labels = np.zeros((len(training_data)//8, 5), dtype=float)\n",
    "print(\"MF_training_data shape: \", MF_training_data.shape)\n",
    "print(\"MF_training_labels shape: \", MF_training_data_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(training_data))\n",
    "print(len(training_data)//8 * 8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, data in enumerate(training_data[0:(len(training_data)//8)]):\n",
    "    layer = MF_Layer(data, user_matrix, movie_matrix, user_ids)\n",
    "    MF_training_data[i] = layer\n",
    "    MF_training_data_labels[i] = nf_label_transformation_function(data[2])\n",
    "print(\"MF_training_data shape: \", MF_training_data.shape)\n",
    "print(\"MF_training_labels shape: \", MF_training_data_labels.shape)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for i, data in enumerate(validation_data[0:len(validation_data)//16]):\n",
    "    layer = MF_Layer(data, user_matrix, movie_matrix, user_ids)\n",
    "    MF_validation_data[i] = layer\n",
    "    MF_validation_data_labels = nf_label_transformation_function(data[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = h5py.File(\"../data/MF_training_data_1.hdf5\", \"w\")\n",
    "f.create_dataset(\"MF_training_data\", data = MF_training_data)\n",
    "f.create_dataset(\"MF_training_data_labels\", data = MF_training_data_labels)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "f = h5py.File(\"../data/MF_training_data_1.hdf5\", \"r\")\n",
    "MF_training_data = f[\"MF_training_data\"][:]\n",
    "MF_training_data_labels = f[\"MF_training_data_labels\"][:]\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 50      \n",
    "def nf_label_transformation_function(label):\n",
    "    \n",
    "    one_hot = np.zeros(5)\n",
    "    one_hot[label - 1] = 1\n",
    "    return one_hot\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "import keras\n",
    "#import random\n",
    "import numpy as np\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from generator_helper_functions import  latent_user_input_layer, Generate_MF_Feedforward, MF_Layer\n",
    "\n",
    "#opt = keras.optimizers.rmsprop()\n",
    "opt = keras.optimizers.SGD()\n",
    "\n",
    "\n",
    "# k dim user vector + number of movies + statistics + month, day, year\n",
    "input_size = k + 75\n",
    "\n",
    "batch_size = 256\n",
    "epochs = 10\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(input_size, activation='relu', input_shape=(input_size,)))\n",
    "model.add(Dense(input_size, activation='relu'))\n",
    "model.add(Dense(input_size // 2, activation='relu'))\n",
    "model.add(Dense(input_size // 4, activation='relu'))\n",
    "model.add(Dense(10, activation='relu'))\n",
    "model.add(Dense(5, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "             optimizer='sgd',\n",
    "             metrics=['mae', 'acc'])\n",
    "\n",
    "print(\"Model Compiled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "cwd = os.getcwd()\n",
    "print(cwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('../models/movie_user_MF_V2_cat_continued_data7.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpointer = ModelCheckpoint(filepath='../models/movie_user_MF_V2_cat_continued_round2_data1.hdf5', verbose=1, save_best_only=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "model.fit_generator(train_gen,\n",
    "                    epochs=5,\n",
    "                    steps_per_epoch=1000,\n",
    "                    validation_data=validation_gen,\n",
    "                    validation_steps=100,\n",
    "                    callbacks=[checkpointer]\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(MF_training_data, \n",
    "          MF_training_data_labels,\n",
    "          epochs=epochs,\n",
    "          batch_size=batch_size,\n",
    "          verbose=2,\n",
    "          validation_split=.1,\n",
    "          callbacks=[checkpointer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reverse_one_hot(label):\n",
    "    for i in range(5):\n",
    "        if label[i] == 1:\n",
    "            return i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "precision_score(\n",
    "    list(\n",
    "        map(lambda x: \n",
    "            np.argmax(model.predict(np.expand_dims(x, axis=0))), MF_training_data[0:100])), \n",
    "    list(map(lambda x: np.argmax(x), MF_training_data_labels[0:100])), average='macro')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"p:\\t\\t\", list(\n",
    "        map(lambda x: np.argmax(model.predict(np.expand_dims(x, axis=0))) + 1, MF_training_data[0:10])))\n",
    "print(\"a:\\t\\t\", list(map(lambda x: np.argmax(x) + 1, MF_training_data_labels[0:10])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate_generator(validation_gen,\n",
    "                    steps=1000,\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_data[0:30, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
