{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/steich/NF_Prize_Data/nf_prize_env/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded from file\n",
      "Training Samples:  90432456 | Validation Samples:  10048051\n",
      "Training and Validation datasets separated\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import random\n",
    "import pdb\n",
    "from generator_helper_functions import create_user_maxtrix, create_user_ids, latent_user_input_layer\n",
    "\n",
    "f = h5py.File('../data/Full_NF.h5', 'r')\n",
    "dset = f['dataset_1']\n",
    "dset = dset[:]\n",
    "print(\"Data loaded from file\")\n",
    "\n",
    "dset_indicies = set(range(len(dset)))\n",
    "train_num = int(.9 * len(dset))\n",
    "print(\"Training Samples: \", train_num, \"| Validation Samples: \", len(dset) - train_num)\n",
    "\n",
    "user_ids = create_user_ids(dset)\n",
    "train_indicies = random.sample(dset_indicies, train_num)\n",
    "validation_indicies = dset_indicies.difference(train_indicies)\n",
    "training_data = np.array([dset[i] for i in train_indicies])\n",
    "del train_indicies\n",
    "validation_data = np.array([dset[i] for i in validation_indicies])\n",
    "del validation_indicies\n",
    "print(\"Training and Validation datasets separated\")\n",
    "#del dset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_ids = create_user_ids(dset)\n",
    "k = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_indices[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "del dset_indicies\n",
    "#del train_indicies\n",
    "#del validation_indicies\n",
    "del dset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_matrix = create_user_maxtrix(training_data, user_ids, k_n=k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/user_matrix.pickle', 'wb') as handle:\n",
    "    pickle.dump(user_matrix, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open('../data/training_data.pickle', 'wb') as handle:\n",
    "    pickle.dump(training_data, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open('../data/validation_data.pickle', 'wb') as handle:\n",
    "    pickle.dump(validation_data, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open('../data/user_ids.pickle', 'wb') as handle:\n",
    "    pickle.dump(user_ids, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "del user_matrix\n",
    "del training_data\n",
    "del validation_data\n",
    "del user_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('../data/user_matrix.pickle', 'rb') as handle:\n",
    "    user_matrix = pickle.load(handle)\n",
    "with open('../data/training_data.pickle', 'rb') as handle:\n",
    "    training_data = pickle.load(handle)\n",
    "with open('../data/validation_data.pickle', 'rb') as handle:\n",
    "    validation_data = pickle.load(handle)\n",
    "with open('../data/user_ids.pickle', 'rb') as handle:\n",
    "    user_ids = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from generator_helper_functions import create_user_maxtrix\n",
    "user_matrix = create_user_maxtrix(training_data, user_ids, k_n=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.57182926, 0.60183805, 0.45889142, ..., 0.7786627 , 0.73003244,\n",
       "        0.97921675],\n",
       "       [0.5277477 , 0.53475094, 0.4299253 , ..., 0.87229574, 0.43106568,\n",
       "        0.67818606],\n",
       "       [0.5079858 , 0.66573393, 0.50564766, ..., 0.7727094 , 0.70960677,\n",
       "        0.7534699 ],\n",
       "       ...,\n",
       "       [0.5052166 , 0.5821171 , 0.4181541 , ..., 0.7926306 , 0.77288043,\n",
       "        0.9436375 ],\n",
       "       [0.56737584, 0.62411636, 0.46938246, ..., 0.7722611 , 0.69581765,\n",
       "        0.99700797],\n",
       "       [0.56497455, 0.62441105, 0.48783594, ..., 0.77094686, 0.70339984,\n",
       "        0.98711175]], dtype=float32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer = MinMaxScaler(copy=False)\n",
    "transformer.fit_transform(user_matrix)\n",
    "user_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Compiled\n"
     ]
    }
   ],
   "source": [
    "k = 50      \n",
    "def nf_label_transformation_function(label):\n",
    "    \n",
    "    one_hot = np.zeros(5)\n",
    "    one_hot[label - 1] = 1\n",
    "    return one_hot\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "import keras\n",
    "#import random\n",
    "import numpy as np\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from generator_helper_functions import  latent_user_input_layer, Generate_User_Latent_Feedforward\n",
    "\n",
    "#opt = keras.optimizers.rmsprop()\n",
    "opt = keras.optimizers.SGD()\n",
    "\n",
    "\n",
    "# k dim user vector + number of movies + statistics + month, day, year\n",
    "input_size = k + 17770\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(input_size, activation='relu', input_shape=(input_size,)))\n",
    "model.add(Dense(1000, activation='relu'))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(10, activation='relu'))\n",
    "model.add(Dense(5, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "             optimizer='sgd',\n",
    "             metrics=['mse', 'mae', 'acc'])\n",
    "\n",
    "print(\"Model Compiled\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "k = 50        \n",
    "def nf_label_transformation_function(label):\n",
    "    \n",
    "    one_hot = np.zeros(5)\n",
    "    one_hot[label - 1] = 1\n",
    "    return one_hot\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "import keras\n",
    "#import random\n",
    "import numpy as np\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from generator_helper_functions import  latent_user_input_layer, Generate_User_Latent_Feedforward\n",
    "\n",
    "#opt = keras.optimizers.rmsprop()\n",
    "opt = keras.optimizers.SGD()\n",
    "\n",
    "\n",
    "# k dim user vector + number of movies + statistics + month, day, year\n",
    "input_size = k + 17770\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(input_size//2, activation='relu', input_shape=(input_size,)))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(1, activation='linear'))\n",
    "\n",
    "model.compile(loss='mae',\n",
    "             optimizer='sgd',\n",
    "             metrics=['mse', 'acc'])\n",
    "\n",
    "print(\"Model Compiled\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "input_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "1000/1000 [==============================] - 1900s 2s/step - loss: 1.4391 - mean_squared_error: 0.1481 - mean_absolute_error: 0.2966 - acc: 0.3355 - val_loss: 1.4384 - val_mean_squared_error: 0.1479 - val_mean_absolute_error: 0.2960 - val_acc: 0.3425\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.43835, saving model to ../models/latent_user_SGD_MSLE_K50_cat_v1__normalized.hdf5\n",
      "Epoch 2/4\n",
      "1000/1000 [==============================] - 5674s 6s/step - loss: 1.4368 - mean_squared_error: 0.1479 - mean_absolute_error: 0.2958 - acc: 0.3378 - val_loss: 1.4282 - val_mean_squared_error: 0.1473 - val_mean_absolute_error: 0.2946 - val_acc: 0.3384\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.43835 to 1.42821, saving model to ../models/latent_user_SGD_MSLE_K50_cat_v1__normalized.hdf5\n",
      "Epoch 3/4\n",
      "1000/1000 [==============================] - 1884s 2s/step - loss: 1.4359 - mean_squared_error: 0.1479 - mean_absolute_error: 0.2959 - acc: 0.3340 - val_loss: 1.4187 - val_mean_squared_error: 0.1468 - val_mean_absolute_error: 0.2945 - val_acc: 0.3430\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.42821 to 1.41872, saving model to ../models/latent_user_SGD_MSLE_K50_cat_v1__normalized.hdf5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe9d3e3f828>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def nf_label_transformation_function(label):\n",
    "    \n",
    "    one_hot = np.zeros(5)\n",
    "    one_hot[label - 1] = 1\n",
    "    return one_hot\n",
    "\n",
    "train_gen = Generate_User_Latent_Feedforward(training_data, \n",
    "                                batch_size, \n",
    "                                (input_size,), \n",
    "                                (5,), #label shape\n",
    "                                latent_user_input_layer, \n",
    "                                lambda x: nf_label_transformation_function(x[2]),\n",
    "                                user_matrix,\n",
    "                                user_ids,\n",
    "                                normalized = True\n",
    "                                )\n",
    "\n",
    "validation_gen = Generate_User_Latent_Feedforward(validation_data, \n",
    "                                batch_size, \n",
    "                                (input_size,), \n",
    "                                (5,), #label shape\n",
    "                                latent_user_input_layer, \n",
    "                                lambda x: nf_label_transformation_function(x[2]),\n",
    "                                user_matrix,\n",
    "                                user_ids,\n",
    "                                normalized = True\n",
    "                                )\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='../models/latent_user_SGD_MSLE_K50_cat_v1__normalized.hdf5', verbose=1, save_best_only=True)\n",
    "model.fit_generator(train_gen,\n",
    "                    epochs=4,\n",
    "                    steps_per_epoch=1000,\n",
    "                    validation_data=validation_gen,\n",
    "                    validation_steps=100,\n",
    "                    callbacks=[checkpointer]\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    6042,   173633,        4, 20051106])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_score(\n",
    "    list(map(lambda x: round(float(model.predict(np.expand_dims(latent_user_input_layer(x, user_matrix, user_ids, True), axis=0)) * 5)), \n",
    "             validation_data[0:3000])), \n",
    "    validation_data[0:3000,2], average='micro') \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_data[0:3, 2]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "dset = f['nf_data']\n",
    "dset = dset[:]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "dset[:,1][0:3]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "user_ids = list(set(dset[:,1]))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "del dset"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "movie_ids = list(range(0, 17771))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "training_data[training_data[:,1].argsort()]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "movie_ids[-1]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from scipy.sparse import *\n",
    "from scipy import *\n",
    "user_rating_matrix = lil_matrix( (len(user_ids), 17771), dtype=int8 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_ids = {user:i for i, user in enumerate(user_ids)}"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for i in training_data:\n",
    "    user_rating_matrix[user_ids[i[1]], i[0]] = i[2]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def add_user_rating(i):\n",
    "    user_rating_matrix[user_ids[i[1]], i[0]] = i[2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_rating_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_rating_matrix"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%%capture\n",
    "list(map(add_user_rating, training_data[:]))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "user_rating_matrix"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "sort_test = training_data[0:10]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from scipy.sparse.linalg import svds"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "user_rating_matrix = user_rating_matrix.tocsr()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "U, sigma, Vt = svds(user_rating_matrix.asfptype(), k = 50)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "user_group_vectors = U * sigma"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "user_group_vectors[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
