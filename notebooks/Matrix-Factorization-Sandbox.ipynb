{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/steich/NF_Prize_Data/nf_prize_env/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/name_one_hot_dict.pickle'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-680f6f792ae3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgenerator_helper_functions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcreate_user_maxtrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_user_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlatent_user_input_layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/Full_NF.h5'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/NF_Prize_Data/generator_helper_functions.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msvds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/name_one_hot_dict.pickle'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mname_one_hot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/name_one_hot_dict.pickle'"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import random\n",
    "import pdb\n",
    "from generator_helper_functions import create_user_maxtrix, create_user_ids, latent_user_input_layer\n",
    "\n",
    "f = h5py.File('data/Full_NF.h5', 'r')\n",
    "dset = f['nf_data']\n",
    "dset = dset[:]\n",
    "print(\"Data loaded from file\")\n",
    "\n",
    "dset_indicies = set(range(len(dset)))\n",
    "train_num = int(.9 * len(dset))\n",
    "print(\"Training Samples: \", train_num, \"| Validation Samples: \", len(dset) - train_num)\n",
    "\n",
    "train_indicies = random.sample(dset_indicies, train_num)\n",
    "validation_indicies = dset_indicies.difference(train_indicies)\n",
    "training_data = np.array([dset[i] for i in train_indicies])\n",
    "validation_data = np.array([dset[i] for i in validation_indicies])\n",
    "print(\"Training and Validation datasets separated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_ids = create_user_ids(dset)\n",
    "k = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del dset_indicies\n",
    "del train_indicies\n",
    "del validation_indicies\n",
    "del dset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/user_matrix.pickle', 'wb') as handle:\n",
    "    pickle.dump(user_matrix, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open('data/training_data.pickle', 'wb') as handle:\n",
    "    pickle.dump(training_data, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open('data/validation_data.pickle', 'wb') as handle:\n",
    "    pickle.dump(validation_data, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open('data/user_ids.pickle', 'wb') as handle:\n",
    "    pickle.dump(user_ids, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del user_matrix\n",
    "del training_data\n",
    "del validation_data\n",
    "del user_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('data/user_matrix.pickle', 'rb') as handle:\n",
    "    user_matrix = pickle.load(handle)\n",
    "with open('data/training_data.pickle', 'rb') as handle:\n",
    "    training_data = pickle.load(handle)\n",
    "with open('data/validation_data.pickle', 'rb') as handle:\n",
    "    validation_data = pickle.load(handle)\n",
    "with open('data/user_ids.pickle', 'rb') as handle:\n",
    "    user_ids = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from generator_helper_functions import create_user_maxtrix\n",
    "user_matrix = create_user_maxtrix(training_data, user_ids, k_n=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.45480946, 0.5649543 , 0.54005206, ..., 0.78012514, 0.7245458 ,\n",
       "        0.97671676],\n",
       "       [0.52922237, 0.48295796, 0.55065936, ..., 0.8655711 , 0.40475172,\n",
       "        0.66800594],\n",
       "       [0.5129979 , 0.57189465, 0.4832853 , ..., 0.772317  , 0.7031396 ,\n",
       "        0.74582773],\n",
       "       ...,\n",
       "       [0.5170382 , 0.5100255 , 0.53248763, ..., 0.7930147 , 0.7573515 ,\n",
       "        0.9455885 ],\n",
       "       [0.47609553, 0.5598326 , 0.5154664 , ..., 0.770388  , 0.6871689 ,\n",
       "        0.9969276 ],\n",
       "       [0.46394268, 0.54970384, 0.49299762, ..., 0.7700641 , 0.6939482 ,\n",
       "        0.98722965]], dtype=float32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer = MinMaxScaler(copy=False)\n",
    "transformer.fit_transform(user_matrix)\n",
    "user_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Compiled\n"
     ]
    }
   ],
   "source": [
    "k = 50      \n",
    "def nf_label_transformation_function(label):\n",
    "    \n",
    "    one_hot = np.zeros(5)\n",
    "    one_hot[label - 1] = 1\n",
    "    return one_hot\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "import keras\n",
    "#import random\n",
    "import numpy as np\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from generator_helper_functions import  latent_user_input_layer, Generate_User_Latent_Feedforward\n",
    "\n",
    "#opt = keras.optimizers.rmsprop()\n",
    "opt = keras.optimizers.SGD()\n",
    "\n",
    "\n",
    "# k dim user vector + number of movies + statistics + month, day, year\n",
    "input_size = k + 17770\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(input_size, activation='relu', input_shape=(input_size,)))\n",
    "model.add(Dense(1000, activation='relu'))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(10, activation='relu'))\n",
    "model.add(Dense(1, activation='linear'))\n",
    "\n",
    "model.compile(loss='mean_squared_logarithmic_error',\n",
    "             optimizer='sgd',\n",
    "             metrics=['mse', 'mae', 'acc'])\n",
    "\n",
    "print(\"Model Compiled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 50        \n",
    "def nf_label_transformation_function(label):\n",
    "    \n",
    "    one_hot = np.zeros(5)\n",
    "    one_hot[label - 1] = 1\n",
    "    return one_hot\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "import keras\n",
    "#import random\n",
    "import numpy as np\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from generator_helper_functions import  latent_user_input_layer, Generate_User_Latent_Feedforward\n",
    "\n",
    "#opt = keras.optimizers.rmsprop()\n",
    "opt = keras.optimizers.SGD()\n",
    "\n",
    "\n",
    "# k dim user vector + number of movies + statistics + month, day, year\n",
    "input_size = k + 17770\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(input_size//2, activation='relu', input_shape=(input_size,)))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(1, activation='linear'))\n",
    "\n",
    "model.compile(loss='mae',\n",
    "             optimizer='sgd',\n",
    "             metrics=['mse', 'acc'])\n",
    "\n",
    "print(\"Model Compiled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "1000/1000 [==============================] - 1905s 2s/step - loss: 0.0852 - mean_squared_error: 1.3194 - mean_absolute_error: 0.9585 - acc: 0.2945 - val_loss: 0.0754 - val_mean_squared_error: 1.2120 - val_mean_absolute_error: 0.9308 - val_acc: 0.2992\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.07542, saving model to models/latent_user_SGD_MSLE_K50_v1__normalized.hdf5\n",
      "Epoch 2/2\n",
      " 393/1000 [==========>...................] - ETA: 18:21 - loss: 0.0736 - mean_squared_error: 1.2052 - mean_absolute_error: 0.9291 - acc: 0.3034"
     ]
    }
   ],
   "source": [
    "train_gen = Generate_User_Latent_Feedforward(training_data, \n",
    "                                batch_size, \n",
    "                                (input_size,), \n",
    "                                (1,), #label shape\n",
    "                                latent_user_input_layer, \n",
    "                                lambda x: x[2],\n",
    "                                user_matrix,\n",
    "                                user_ids,\n",
    "                                normalized = True\n",
    "                                )\n",
    "\n",
    "validation_gen = Generate_User_Latent_Feedforward(validation_data, \n",
    "                                batch_size, \n",
    "                                (input_size,), \n",
    "                                (1,), #label shape\n",
    "                                latent_user_input_layer, \n",
    "                                lambda x: x[2],\n",
    "                                user_matrix,\n",
    "                                user_ids,\n",
    "                                normalized = True\n",
    "                                )\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='models/latent_user_SGD_MSLE_K50_v1__normalized.hdf5', verbose=1, save_best_only=True)\n",
    "model.fit_generator(train_gen,\n",
    "                    epochs=2,\n",
    "                    steps_per_epoch=1000,\n",
    "                    validation_data=validation_gen,\n",
    "                    validation_steps=100,\n",
    "                    callbacks=[checkpointer]\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   12238,   571810,        4, 20051206])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_score(\n",
    "    list(map(lambda x: round(float(model.predict(np.expand_dims(latent_user_input_layer(x, user_matrix, user_ids, True), axis=0)) * 5)), \n",
    "             validation_data[0:3000])), \n",
    "    validation_data[0:3000,2], average='micro') \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_data[0:3, 2]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "dset = f['nf_data']\n",
    "dset = dset[:]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "dset[:,1][0:3]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "user_ids = list(set(dset[:,1]))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "del dset"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "movie_ids = list(range(0, 17771))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "training_data[training_data[:,1].argsort()]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "movie_ids[-1]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from scipy.sparse import *\n",
    "from scipy import *\n",
    "user_rating_matrix = lil_matrix( (len(user_ids), 17771), dtype=int8 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_ids = {user:i for i, user in enumerate(user_ids)}"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for i in training_data:\n",
    "    user_rating_matrix[user_ids[i[1]], i[0]] = i[2]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def add_user_rating(i):\n",
    "    user_rating_matrix[user_ids[i[1]], i[0]] = i[2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_rating_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_rating_matrix"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%%capture\n",
    "list(map(add_user_rating, training_data[:]))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "user_rating_matrix"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "sort_test = training_data[0:10]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from scipy.sparse.linalg import svds"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "user_rating_matrix = user_rating_matrix.tocsr()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "U, sigma, Vt = svds(user_rating_matrix.asfptype(), k = 50)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "user_group_vectors = U * sigma"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "user_group_vectors[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
